{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Working With Remote Servers\n",
    "\n",
    "This tutorial will introduce how to start a remote Hangar server, and how to work with ``remotes`` from the client side. \n",
    "\n",
    "Particular attention is paid to the concept of a ***partially fetch* / *partial clone*** operations. This is a key component of the Hangar design which provides the ability to quickly and efficiently work with data contained in remote repositories whose full size would be significatly prohibitive to local use under most circumstances. \n",
    "\n",
    "*Note:*\n",
    "\n",
    "> At the time of writing, the API, user-facing functionality, client-server negotiation protocols, and test coverage of the remotes implementation is generally adqequate for this to serve as an \"alpha\" quality preview. However, please be warned that significantly less time has been spent in this module to optimize speed, refactor for simplicity, and assure stability under heavy loads than the rest of the Hangar core. While we can guarrentee that your data is secure on disk, you may experience crashes from time to time when working with remotes. In addition, sending data over the wire should NOT be considered secure in ANY way. No in-transit encryption, user authentication, or secure access limitations are implemented at this moment. We realize the importance of these types of protections, and they are on our radar for the next release cycle. If you are interested in making a contribution to Hangar, this module contains a lot of low hanging fruit which would would provide drastic improvements and act as a good intro the the internal Hangar data model. Please get in touch with us to discuss!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting a Hangar Server\n",
    "\n",
    "To start a hangar server, navigate to the command line and simply execute:\n",
    "\n",
    "```\n",
    "$ hangar server\n",
    "```\n",
    "\n",
    "This will get a local server instanse running at `localhost:50051`. The IP and port can be configured by setting the `--ip` and `--port` flags to the desired values at the command line\n",
    "\n",
    "A blocking process will begin in that terminal session. Leave it running while you experiment with connecting from a client repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Remotes with a Local Repository\n",
    "\n",
    "The CLI is the easiest way to interact with the remote server from a local repository (Though all functioanlity is mirrorred via the repository API (more on that later).\n",
    "\n",
    "Before we begin we will set up a repository with some data, a few commits, two branches, and a merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup a Test Repo\n",
    "\n",
    "As normal, we shall begin with creating a repository and adding some data. This should be familiar to you from previous tutorials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hangar import Repository\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "testData = np.loadtxt('/Users/rick/projects/tensorwerk/hangar/dev/data/dota2Dataset/dota2Test.csv', delimiter=',', dtype=np.uint8)\n",
    "trainData = np.loadtxt('/Users/rick/projects/tensorwerk/hangar/dev/data/dota2Dataset/dota2Train.csv', delimiter=',', dtype=np.uint16)\n",
    "\n",
    "testName = 'test'\n",
    "testPrototype = testData[0]\n",
    "trainName = 'train'\n",
    "trainPrototype = trainData[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hangar Repo initialized at: /Users/rick/projects/tensorwerk/hangar/dev/intro/.hangar\n"
     ]
    }
   ],
   "source": [
    "repo = Repository('/Users/rick/projects/tensorwerk/hangar/dev/intro/')\n",
    "repo.init(user_name='Rick Izzo', user_email='rick@tensorwerk.com', remove_old=True)\n",
    "co = repo.checkout(write=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10500it [00:00, 22604.66it/s]                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* 9b93b393e8852a1fa57f0170f54b30c2c0c7d90f (\u001b[1;31madd-train\u001b[m) (\u001b[1;31mmaster\u001b[m) : initial commit on master with test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "co.arraysets.init_arrayset(name=testName, prototype=testPrototype, named_samples=False)\n",
    "testaset = co.arraysets[testName]\n",
    "\n",
    "pbar = tqdm(total=testData.shape[0])\n",
    "with testaset as ds:\n",
    "    for gameIdx, gameData in enumerate(testData):\n",
    "        if (gameIdx % 500 == 0):\n",
    "            pbar.update(500)\n",
    "        ds.add(gameData)\n",
    "pbar.close()\n",
    "\n",
    "co.commit('initial commit on master with test data')\n",
    "\n",
    "repo.create_branch('add-train')\n",
    "co.close()\n",
    "repo.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "93000it [00:03, 23300.08it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* 903fa337a6d1925f82a1700ad76f6c074eec8d7b (\u001b[1;31madd-train\u001b[m) : added training data on another branch\n",
      "* 9b93b393e8852a1fa57f0170f54b30c2c0c7d90f (\u001b[1;31mmaster\u001b[m) : initial commit on master with test data\n"
     ]
    }
   ],
   "source": [
    "co = repo.checkout(write=True, branch='add-train')\n",
    "\n",
    "co.arraysets.init_arrayset(name=trainName, prototype=trainPrototype, named_samples=False)\n",
    "trainaset = co.arraysets[trainName]\n",
    "\n",
    "pbar = tqdm(total=trainData.shape[0])\n",
    "with trainaset as dt:\n",
    "    for gameIdx, gameData in enumerate(trainData):\n",
    "        if (gameIdx % 500 == 0):\n",
    "            pbar.update(500)\n",
    "        dt.add(gameData)\n",
    "pbar.close()\n",
    "\n",
    "co.commit('added training data on another branch')\n",
    "co.close()\n",
    "repo.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* b119a4db817d9a4120593938ee4115402aa1405f (\u001b[1;31mmaster\u001b[m) : more changes here\n",
      "* 9b93b393e8852a1fa57f0170f54b30c2c0c7d90f : initial commit on master with test data\n"
     ]
    }
   ],
   "source": [
    "co = repo.checkout(write=True, branch='master')\n",
    "co.metadata['earaea'] = 'eara'\n",
    "co.commit('more changes here')\n",
    "co.close()\n",
    "repo.log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pushing to a Remote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the API to add a remote, however, this can also be done wtih the CLI command:\n",
    "\n",
    "    $ hangar remote add origin localhost:50051"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RemoteInfo(name='origin', address='localhost:50051')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo.remote.add('origin', 'localhost:50051')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pushing is as simple as running a simple command from the CLI or API:\n",
    "\n",
    "    $ hangar push origin master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Push the \"master\" branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "counting objects: 100%|██████████| 2/2 [00:00<00:00, 13.31it/s]\n",
      "pushing schemas: 100%|██████████| 1/1 [00:00<00:00, 273.60it/s]\n",
      "pushing data: 10295it [00:00, 27622.34it/s]                          \n",
      "pushing metadata: 100%|██████████| 1/1 [00:00<00:00, 510.94it/s]\n",
      "pushing commit refs: 100%|██████████| 2/2 [00:00<00:00, 36.52it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'master'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo.remote.push('origin', 'master')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Push the \"add-train\" branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "counting objects: 100%|██████████| 1/1 [00:00<00:00,  1.14it/s]\n",
      "pushing schemas: 100%|██████████| 1/1 [00:00<00:00, 464.02it/s]\n",
      "pushing data: 92651it [00:04, 6427.60it/s]                            \n",
      "pushing metadata: 0it [00:00, ?it/s]\n",
      "pushing commit refs: 100%|██████████| 1/1 [00:00<00:00,  3.68it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'add-train'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo.remote.push('origin', 'add-train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Details of the Negotiation Processs\n",
    "\n",
    "*(The following details are not necessary to use the system, but may be of interest to some readers)*\n",
    "\n",
    "When we push data, **we perform a negotation with the server** which basically occurs like this:\n",
    "\n",
    "\n",
    "- Hi, I would like to push this branch, do you have it?\n",
    "\n",
    "  - If yes, what is the latest commit you record on it?\n",
    "  \n",
    "    - Is that the same commit I'm trying to push? if yes, abort\n",
    "    \n",
    "    - Is that a commit I dont have? If yes, someone else has updated that branch, abort\n",
    "    \n",
    "- Here's the commit digests which are parents of my branches head, which commits are you missing?\n",
    "\n",
    "- Ok great, I'm going to scan through each of those commits to find the data hashes they contain. Tell me which ones you are missing.\n",
    "\n",
    "- Thanks, now I'll send you all of the data corresponding to those hashes. It might be a lot of data, so we'll handle this in batches so that if my connection cuts out, we can resume this later\n",
    "\n",
    "- Now that you have the data, I'm going to send the actual commit references for you to store, this isn't that much information, but you'll be sure to verify that I'm not trying to pull any funny buisness and send you incorrect data.\n",
    "\n",
    "- Now that you've recieved everything, and have verified it matches what I told you it is, go ahead and make those commits I've pushed `available` as the `HEAD` of the branch I just sent. It's some good work that others will want!\n",
    "\n",
    "\n",
    "When we want to fetch updates to a branch, essentially the exact same thing happens in reverse. Instead of asking the server what it doesn't have, we ask it what it does have, and then request the stuff that we are missing!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial Fetching and Clones\n",
    "\n",
    "**Now we will introduce one of the most important and unique features of Hangar remotes: Partial fetch/clone of data!**\n",
    "\n",
    "*There is a very real problem with keeping the full history of data - **it's huge**!* The size of data can very easily exceeds what can fit on (most) contributors laptops or personal workstations. This section explains how Hangar can handle working with arraysets which are prohibitively large to download or store on a single machine.\n",
    "\n",
    "As mentioned in High Performance From Simplicity, under the hood Hangar deals with “Data” and “Bookkeeping” completely separately. We’ve previously covered what exactly we mean by Data in How Hangar Thinks About Data, so we’ll briefly cover the second major component of Hangar here. In short “Bookkeeping” describes everything about the repository. By everything, we do mean that the Bookkeeping records describe everything: all commits, parents, branches, arraysets, samples, data descriptors, schemas, commit message, etc. Though complete, these records are fairly small (tens of MB in size for decently sized repositories with decent history), and are highly compressed for fast transfer between a Hangar client/server.\n",
    "\n",
    "A brief technical interlude\n",
    "\n",
    "> There is one very important (and rather complex) property which gives Hangar Bookeeping massive power: Existence of some data piece is always known to Hangar and stored immutably once committed. However, the access pattern, backend, and locating information for this data piece may (and over time, will) be unique in every hangar repository instance.\n",
    ">\n",
    "> Though the details of how this works is well beyond the scope of this document, the following example may provide some insight into the implications of this property:\n",
    "> \n",
    "> If you clone some hangar repository, Bookeeping says that “some number of data pieces exist” and they should retrieved from the server. However, the bookeeping records transfered in a fetch / push / clone operation do not include information about where that piece of data existed on the client (or server) computer. Two synced repositories can use completly different backends to store the data, in completly different locations, and it does not matter - Hangar only guarrentees that when collaborators ask for a data sample in some checkout, that they will be provided with identical arrays, not that they will come from the same place or be stored in the same way. Only when data is actually retrieved is the “locating information” set for that repository instance.\n",
    "Because Hangar makes no assumptions about how/where it should retrieve some piece of data, or even an assumption that it exists on the local machine, and because records are small and completely describe history, once a machine has the Bookkeeping, it can decide what data it actually wants to materialize on it’s local disk! These partial fetch / partial clone operations can materialize any desired data, whether it be for a few records at the head branch, for all data in a commit, or for the entire historical data. A future release will even include the ability to stream data directly to a hangar checkout and materialize the data in memory without having to save it to disk at all!\n",
    "\n",
    "More importantly: Since Bookkeeping describes all history, merging can be performed between branches which may contain partial (or even no) actual data. Aka. You don’t need data on disk to merge changes into it. It’s an odd concept which will be shown in this tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloning a Remote Repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    $ hangar clone localhost:50051"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloneRepo = Repository('/Users/rick/projects/tensorwerk/hangar/dev/dota-clone/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we perform the initial clone, we will only recieve the \"master\" branch by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fetching commit data refs:  50%|█████     | 1/2 [00:00<00:00,  7.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hangar Repo initialized at: /Users/rick/projects/tensorwerk/hangar/dev/dota-clone/.hangar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fetching commit data refs: 100%|██████████| 2/2 [00:00<00:00,  8.84it/s]\n",
      "fetching commit spec: 100%|██████████| 2/2 [00:00<00:00, 26.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard reset requested with writer_lock: 893d3a43-7f95-44e4-9fed-72feb3cf49df\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'master'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cloneRepo.clone('rick izzo', 'rick@tensorwerk.com', 'localhost:50051', remove_old=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* b119a4db817d9a4120593938ee4115402aa1405f (\u001b[1;31mmaster\u001b[m) (\u001b[1;31morigin/master\u001b[m) : more changes here\n",
      "* 9b93b393e8852a1fa57f0170f54b30c2c0c7d90f : initial commit on master with test data\n"
     ]
    }
   ],
   "source": [
    "cloneRepo.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['master', 'origin/master']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cloneRepo.list_branches()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the \"add-train\" branch, we `fetch` it from the remote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fetching commit data refs: 100%|██████████| 1/1 [00:01<00:00,  1.02s/it]\n",
      "fetching commit spec: 100%|██████████| 1/1 [00:00<00:00,  3.69it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'origin/add-train'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cloneRepo.remote.fetch('origin', 'add-train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['master', 'origin/add-train', 'origin/master']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cloneRepo.list_branches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* 903fa337a6d1925f82a1700ad76f6c074eec8d7b (\u001b[1;31morigin/add-train\u001b[m) : added training data on another branch\n",
      "* 9b93b393e8852a1fa57f0170f54b30c2c0c7d90f : initial commit on master with test data\n"
     ]
    }
   ],
   "source": [
    "cloneRepo.log(branch='origin/add-train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a local branch from the `origin/add-train` branch, just like in `Git`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'add-train'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cloneRepo.create_branch('add-train', '903fa337a6d1925f82a1700ad76f6c074eec8d7b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['add-train', 'master', 'origin/add-train', 'origin/master']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cloneRepo.list_branches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* 903fa337a6d1925f82a1700ad76f6c074eec8d7b (\u001b[1;31madd-train\u001b[m) (\u001b[1;31morigin/add-train\u001b[m) : added training data on another branch\n",
      "* 9b93b393e8852a1fa57f0170f54b30c2c0c7d90f : initial commit on master with test data\n"
     ]
    }
   ],
   "source": [
    "cloneRepo.log(branch='add-train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking out a Parial Clone/Fetch\n",
    "\n",
    "When we `fetch`/`clone`, the transfers are very quick, because only the commit records/history were retrieved. The data was not sent, because it may be very large to get the entire data across all of history.\n",
    "\n",
    "When you check out a commit with partial data, you will be shown a warning indicating that some data is not available locally. An error is raised if you try to access that particular sample data. Otherwise, everything will appear as normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Checking out BRANCH: master with current HEAD: b119a4db817d9a4120593938ee4115402aa1405f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rick/projects/tensorwerk/hangar/hangar-py/src/hangar/arrayset.py:115: UserWarning: Arrayset: test contains `reference-only` samples, with actual data residing on a remote server. A `fetch-data` operation is required to access these samples.\n",
      "  f'operation is required to access these samples.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "co = cloneRepo.checkout(branch='master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hangar ReaderCheckout                \n",
       "    Writer       : False                \n",
       "    Commit Hash  : b119a4db817d9a4120593938ee4115402aa1405f                \n",
       "    Num Arraysets : 1                \n",
       "    Num Metadata : 1\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see from the `repr` that the arraysets contain partial remote references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hangar Arraysets                \n",
       "    Writeable: False                \n",
       "    Arrayset Names / Partial Remote References:                \n",
       "      - test / True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co.arraysets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hangar ArraysetDataReader                 \n",
       "    Arrayset Name             : test                \n",
       "    Schema Hash              : 2bd5a5720bc3                \n",
       "    Variable Shape           : False                \n",
       "    (max) Shape              : (117,)                \n",
       "    Datatype                 : <class 'numpy.uint8'>                \n",
       "    Named Samples            : False                \n",
       "    Access Mode              : r                \n",
       "    Number of Samples        : 10294                \n",
       "    Partial Remote Data Refs : True\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co.arraysets['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "testKey = next(co.arraysets['test'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "data hash spec: <class 'hangar.backends.remote_50.REMOTE_50_DataHashSpec'> does not exist on this machine. Perform a `data-fetch` operation to retrieve it from the remote server.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-7900b0d54ebc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marraysets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtestKey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/projects/tensorwerk/hangar/hangar-py/src/hangar/arrayset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0msample\u001b[0m \u001b[0marray\u001b[0m \u001b[0mdata\u001b[0m \u001b[0mcorresponding\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mprovided\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \"\"\"\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/tensorwerk/hangar/hangar-py/src/hangar/arrayset.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sspecs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/tensorwerk/hangar/hangar-py/src/hangar/backends/remote_50.py\u001b[0m in \u001b[0;36mread_data\u001b[0;34m(self, hashVal)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhashVal\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mREMOTE_50_DataHashSpec\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         raise FileNotFoundError(\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0;34mf'data hash spec: {REMOTE_50_DataHashSpec} does not exist on this machine. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m             f'Perform a `data-fetch` operation to retrieve it from the remote server.')\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: data hash spec: <class 'hangar.backends.remote_50.REMOTE_50_DataHashSpec'> does not exist on this machine. Perform a `data-fetch` operation to retrieve it from the remote server."
     ]
    }
   ],
   "source": [
    "co.arraysets['test'][testKey]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "co.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetching Data from a Remote\n",
    "\n",
    "To retrieve the data, we use the `fetch_data` operation (accessible via the API or `fetch-data` via the CLI). \n",
    "\n",
    "The amount / type of data to retrieve is extremly configurable via the following options\n",
    "\n",
    "```\n",
    "Retrieve the data for some commit which exists in a `partial` state.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    remote : str\n",
    "        name of the remote to pull the data from\n",
    "    branch : str, optional\n",
    "        The name of a branch whose HEAD will be used as the data fetch\n",
    "        point. If None, ``commit`` argument expected, by default None\n",
    "    commit : str, optional\n",
    "        Commit hash to retrieve data for, If None, ``branch`` argument\n",
    "        expected, by default None\n",
    "    arrayset_names : Optional[Sequence[str]]\n",
    "        Names of the arraysets which should be retrieved for the particular\n",
    "        commits, any arraysets not named will not have their data fetched\n",
    "        from the server. Default behavior is to retrieve all arraysets\n",
    "    max_num_bytes : Optional[int]\n",
    "        If you wish to limit the amount of data sent to the local machine,\n",
    "        set a `max_num_bytes` parameter. This will retrieve only this\n",
    "        amount of data from the server to be placed on the local disk.\n",
    "        Default is to retrieve all data regardless of how large.\n",
    "    retrieve_all_history : Optional[bool]\n",
    "        if data should be retrieved for all history accessible by the parents\n",
    "        of this commit HEAD. by default False\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[str]\n",
    "        commit hashs of the data which was returned.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will retrieve all the data on the \"master\" branch, but not on the \"add-train\" branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "counting objects: 100%|██████████| 1/1 [00:00<00:00, 39.35it/s]\n",
      "fetching data: 100%|██████████| 10294/10294 [00:00<00:00, 17452.01it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['b119a4db817d9a4120593938ee4115402aa1405f']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cloneRepo.remote.fetch_data('origin', branch='master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Checking out BRANCH: master with current HEAD: b119a4db817d9a4120593938ee4115402aa1405f\n"
     ]
    }
   ],
   "source": [
    "co = cloneRepo.checkout(branch='master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hangar ReaderCheckout                \n",
       "    Writer       : False                \n",
       "    Commit Hash  : b119a4db817d9a4120593938ee4115402aa1405f                \n",
       "    Num Arraysets : 1                \n",
       "    Num Metadata : 1\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike before, we see that there is no partial references from the `repr`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hangar Arraysets                \n",
       "    Writeable: False                \n",
       "    Arrayset Names / Partial Remote References:                \n",
       "      - test / False"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co.arraysets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hangar ArraysetDataReader                 \n",
       "    Arrayset Name             : test                \n",
       "    Schema Hash              : 2bd5a5720bc3                \n",
       "    Variable Shape           : False                \n",
       "    (max) Shape              : (117,)                \n",
       "    Datatype                 : <class 'numpy.uint8'>                \n",
       "    Named Samples            : False                \n",
       "    Access Mode              : r                \n",
       "    Number of Samples        : 10294                \n",
       "    Partial Remote Data Refs : False\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co.arraysets['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***When we access the data this time, it is available and retrieved as requested!***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([255, 223,   8,   2,   0, 255,   0,   0,   0,   0,   0,   0,   1,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   1,   0,   0,   0, 255,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   1,   0,   0,   0, 255,   0,   0,\n",
       "         0,   0,   0,   0,   0, 255,   0,   0,   0,   0,   1,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0, 255,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "      dtype=uint8)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co['test', testKey]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "co.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working with mixed local/remote checkout Data\n",
    "\n",
    "If we were to checkout the \"add-train\" branch now, we would see that there is no `arrayset \"train\"` data, but there will be data common to the ancestor that \"master\" and \"add-train\" share."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* 903fa337a6d1925f82a1700ad76f6c074eec8d7b (\u001b[1;31madd-train\u001b[m) (\u001b[1;31morigin/add-train\u001b[m) : added training data on another branch\n",
      "* 9b93b393e8852a1fa57f0170f54b30c2c0c7d90f : initial commit on master with test data\n"
     ]
    }
   ],
   "source": [
    "cloneRepo.log('add-train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the common ancestor is commit: `9b93b393e8852a1fa57f0170f54b30c2c0c7d90f`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show that there is no data on the \"add-train\" branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Checking out BRANCH: add-train with current HEAD: 903fa337a6d1925f82a1700ad76f6c074eec8d7b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rick/projects/tensorwerk/hangar/hangar-py/src/hangar/arrayset.py:115: UserWarning: Arrayset: train contains `reference-only` samples, with actual data residing on a remote server. A `fetch-data` operation is required to access these samples.\n",
      "  f'operation is required to access these samples.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "co = cloneRepo.checkout(branch='add-train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hangar ReaderCheckout                \n",
       "    Writer       : False                \n",
       "    Commit Hash  : 903fa337a6d1925f82a1700ad76f6c074eec8d7b                \n",
       "    Num Arraysets : 2                \n",
       "    Num Metadata : 0\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hangar Arraysets                \n",
       "    Writeable: False                \n",
       "    Arrayset Names / Partial Remote References:                \n",
       "      - test / False\n",
       "      - train / True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co.arraysets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([255, 223,   8,   2,   0, 255,   0,   0,   0,   0,   0,   0,   1,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   1,   0,   0,   0, 255,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   1,   0,   0,   0, 255,   0,   0,\n",
       "         0,   0,   0,   0,   0, 255,   0,   0,   0,   0,   1,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0, 255,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "      dtype=uint8)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co['test', testKey]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainKey = next(co.arraysets['train'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "data hash spec: <class 'hangar.backends.remote_50.REMOTE_50_DataHashSpec'> does not exist on this machine. Perform a `data-fetch` operation to retrieve it from the remote server.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-c20187a5b311>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marraysets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrainKey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/projects/tensorwerk/hangar/hangar-py/src/hangar/arrayset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0msample\u001b[0m \u001b[0marray\u001b[0m \u001b[0mdata\u001b[0m \u001b[0mcorresponding\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mprovided\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \"\"\"\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/tensorwerk/hangar/hangar-py/src/hangar/arrayset.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sspecs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/tensorwerk/hangar/hangar-py/src/hangar/backends/remote_50.py\u001b[0m in \u001b[0;36mread_data\u001b[0;34m(self, hashVal)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhashVal\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mREMOTE_50_DataHashSpec\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         raise FileNotFoundError(\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0;34mf'data hash spec: {REMOTE_50_DataHashSpec} does not exist on this machine. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m             f'Perform a `data-fetch` operation to retrieve it from the remote server.')\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: data hash spec: <class 'hangar.backends.remote_50.REMOTE_50_DataHashSpec'> does not exist on this machine. Perform a `data-fetch` operation to retrieve it from the remote server."
     ]
    }
   ],
   "source": [
    "co.arraysets['train'][trainKey]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "co.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging Branches with Parial Data\n",
    "\n",
    "Even though we don't have the actual data references in the `\"add-train\"` branch, it is still possible to merge the two branches!\n",
    "\n",
    "This is possible because Hangar doesn't use the data contents in it's internal model of checkouts/commits, but instead thinks of a checkouts as a sequence of arraysets/metadata/keys & their associated data hashes (which are very small text records; ie. \"bookkeeping\"). To show this in action, lets merge the two branches `\"master\"` (containing all data locally) and `\"add-train\"` (containing parial remote references for the `\"train\"` arrayset) together and push it to the Remote!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* b119a4db817d9a4120593938ee4115402aa1405f (\u001b[1;31mmaster\u001b[m) (\u001b[1;31morigin/master\u001b[m) : more changes here\n",
      "* 9b93b393e8852a1fa57f0170f54b30c2c0c7d90f : initial commit on master with test data\n"
     ]
    }
   ],
   "source": [
    "cloneRepo.log('master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* 903fa337a6d1925f82a1700ad76f6c074eec8d7b (\u001b[1;31madd-train\u001b[m) (\u001b[1;31morigin/add-train\u001b[m) : added training data on another branch\n",
      "* 9b93b393e8852a1fa57f0170f54b30c2c0c7d90f : initial commit on master with test data\n"
     ]
    }
   ],
   "source": [
    "cloneRepo.log('add-train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perform the Merge**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 3-Way Merge Strategy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'71f3bd864919c6e0c5ef95e2e8fb67102a0f94a2'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cloneRepo.merge('merge commit here', 'master', 'add-train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IT WORKED!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   71f3bd864919c6e0c5ef95e2e8fb67102a0f94a2 (\u001b[1;31mmaster\u001b[m) : merge commit here\n",
      "\u001b[1;31m|\u001b[m\u001b[1;32m\\\u001b[m  \n",
      "* \u001b[1;32m|\u001b[m b119a4db817d9a4120593938ee4115402aa1405f (\u001b[1;31morigin/master\u001b[m) : more changes here\n",
      "\u001b[1;32m|\u001b[m * 903fa337a6d1925f82a1700ad76f6c074eec8d7b (\u001b[1;31madd-train\u001b[m) (\u001b[1;31morigin/add-train\u001b[m) : added training data on another branch\n",
      "\u001b[1;32m|\u001b[m\u001b[1;32m/\u001b[m  \n",
      "* 9b93b393e8852a1fa57f0170f54b30c2c0c7d90f : initial commit on master with test data\n"
     ]
    }
   ],
   "source": [
    "cloneRepo.log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the summary of the master commit to check that the contents are what we expect (containing both `test` and `train` arraysets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Contents Contained in Data Repository \n",
      " \n",
      "================== \n",
      "| Repository Info \n",
      "|----------------- \n",
      "|  Base Directory: /Users/rick/projects/tensorwerk/hangar/dev/dota-clone \n",
      "|  Disk Usage: 45.61 MB \n",
      " \n",
      "=================== \n",
      "| Commit Details \n",
      "------------------- \n",
      "|  Commit: 71f3bd864919c6e0c5ef95e2e8fb67102a0f94a2 \n",
      "|  Created: Mon Aug 19 17:41:02 2019 \n",
      "|  By: rick izzo \n",
      "|  Email: rick@tensorwerk.com \n",
      "|  Message: merge commit here \n",
      " \n",
      "================== \n",
      "| DataSets \n",
      "|----------------- \n",
      "|  Number of Named Arraysets: 2 \n",
      "|\n",
      "|  * Arrayset Name: test \n",
      "|    Num Arrays: 10294 \n",
      "|    Details: \n",
      "|    - schema_hash: 2bd5a5720bc3 \n",
      "|    - schema_dtype: 2 \n",
      "|    - schema_is_var: False \n",
      "|    - schema_max_shape: (117,) \n",
      "|    - schema_is_named: False \n",
      "|    - schema_default_backend: 10 \n",
      "|\n",
      "|  * Arrayset Name: train \n",
      "|    Num Arrays: 92650 \n",
      "|    Details: \n",
      "|    - schema_hash: ded1ae23f9af \n",
      "|    - schema_dtype: 4 \n",
      "|    - schema_is_var: False \n",
      "|    - schema_max_shape: (117,) \n",
      "|    - schema_is_named: False \n",
      "|    - schema_default_backend: 10 \n",
      " \n",
      "================== \n",
      "| Metadata: \n",
      "|----------------- \n",
      "|  Number of Keys: 1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cloneRepo.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pushing the Merge back to the Remote\n",
    "\n",
    "To push this merge back to our original copy of the Repository (`repo`), we just push the `\"master\"` branch back to the remote via the API or CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "counting objects: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s]\n",
      "pushing schemas: 0it [00:00, ?it/s]\n",
      "pushing data: 0it [00:00, ?it/s]\n",
      "pushing metadata: 0it [00:00, ?it/s]\n",
      "pushing commit refs: 100%|██████████| 1/1 [00:00<00:00,  3.85it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'master'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cloneRepo.remote.push('origin', 'master')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at our current state of our other instance of the repo `\"repo\"` we see that the merge changes aren't yet propogated to it (since it hasn't fetched from the remote yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* b119a4db817d9a4120593938ee4115402aa1405f (\u001b[1;31mmaster\u001b[m) (\u001b[1;31morigin/master\u001b[m) : more changes here\n",
      "* 9b93b393e8852a1fa57f0170f54b30c2c0c7d90f : initial commit on master with test data\n"
     ]
    }
   ],
   "source": [
    "repo.log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fetch the merged changes, just `fetch` the branch as normal. Like all fetches, this will be a fast operation, as it will be a `partial fetch` operation, not actually transfering the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fetching commit data refs: 100%|██████████| 1/1 [00:00<00:00,  1.24it/s]\n",
      "fetching commit spec: 100%|██████████| 1/1 [00:00<00:00,  3.62it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'origin/master'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo.remote.fetch('origin', 'master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   71f3bd864919c6e0c5ef95e2e8fb67102a0f94a2 (\u001b[1;31morigin/master\u001b[m) : merge commit here\n",
      "\u001b[1;31m|\u001b[m\u001b[1;32m\\\u001b[m  \n",
      "* \u001b[1;32m|\u001b[m b119a4db817d9a4120593938ee4115402aa1405f (\u001b[1;31mmaster\u001b[m) : more changes here\n",
      "\u001b[1;32m|\u001b[m * 903fa337a6d1925f82a1700ad76f6c074eec8d7b (\u001b[1;31madd-train\u001b[m) (\u001b[1;31morigin/add-train\u001b[m) : added training data on another branch\n",
      "\u001b[1;32m|\u001b[m\u001b[1;32m/\u001b[m  \n",
      "* 9b93b393e8852a1fa57f0170f54b30c2c0c7d90f : initial commit on master with test data\n"
     ]
    }
   ],
   "source": [
    "repo.log('origin/master')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To bring our `\"master\"` branch up to date is a simple fast-forward merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Fast-Forward Merge Strategy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'71f3bd864919c6e0c5ef95e2e8fb67102a0f94a2'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo.merge('ff-merge', 'master', 'origin/master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   71f3bd864919c6e0c5ef95e2e8fb67102a0f94a2 (\u001b[1;31mmaster\u001b[m) (\u001b[1;31morigin/master\u001b[m) : merge commit here\n",
      "\u001b[1;31m|\u001b[m\u001b[1;32m\\\u001b[m  \n",
      "* \u001b[1;32m|\u001b[m b119a4db817d9a4120593938ee4115402aa1405f : more changes here\n",
      "\u001b[1;32m|\u001b[m * 903fa337a6d1925f82a1700ad76f6c074eec8d7b (\u001b[1;31madd-train\u001b[m) (\u001b[1;31morigin/add-train\u001b[m) : added training data on another branch\n",
      "\u001b[1;32m|\u001b[m\u001b[1;32m/\u001b[m  \n",
      "* 9b93b393e8852a1fa57f0170f54b30c2c0c7d90f : initial commit on master with test data\n"
     ]
    }
   ],
   "source": [
    "repo.log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Everything is as it should be!** Now, try it out for yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Contents Contained in Data Repository \n",
      " \n",
      "================== \n",
      "| Repository Info \n",
      "|----------------- \n",
      "|  Base Directory: /Users/rick/projects/tensorwerk/hangar/dev/intro \n",
      "|  Disk Usage: 79.98 MB \n",
      " \n",
      "=================== \n",
      "| Commit Details \n",
      "------------------- \n",
      "|  Commit: 71f3bd864919c6e0c5ef95e2e8fb67102a0f94a2 \n",
      "|  Created: Mon Aug 19 17:41:02 2019 \n",
      "|  By: rick izzo \n",
      "|  Email: rick@tensorwerk.com \n",
      "|  Message: merge commit here \n",
      " \n",
      "================== \n",
      "| DataSets \n",
      "|----------------- \n",
      "|  Number of Named Arraysets: 2 \n",
      "|\n",
      "|  * Arrayset Name: test \n",
      "|    Num Arrays: 10294 \n",
      "|    Details: \n",
      "|    - schema_hash: 2bd5a5720bc3 \n",
      "|    - schema_dtype: 2 \n",
      "|    - schema_is_var: False \n",
      "|    - schema_max_shape: (117,) \n",
      "|    - schema_is_named: False \n",
      "|    - schema_default_backend: 10 \n",
      "|\n",
      "|  * Arrayset Name: train \n",
      "|    Num Arrays: 92650 \n",
      "|    Details: \n",
      "|    - schema_hash: ded1ae23f9af \n",
      "|    - schema_dtype: 4 \n",
      "|    - schema_is_var: False \n",
      "|    - schema_max_shape: (117,) \n",
      "|    - schema_is_named: False \n",
      "|    - schema_default_backend: 10 \n",
      " \n",
      "================== \n",
      "| Metadata: \n",
      "|----------------- \n",
      "|  Number of Keys: 1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "repo.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
