{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store and load COCO dataset for Image Captioning\n",
    "\n",
    "This tutorial acts as a step by step guide for fetching, preprocessing, storing and loading the COCO dataset for image captioning using deep learning. We use tensorflow for this tutorial but we have another tutorial with PyTorch in the same hierarchy. This tutorial assumes you have downloaded and extracted the COCO dataset in the current directory. If you haven't below shell commands should help you do it (beware, it's about 14 GB data). \n",
    "\n",
    "\n",
    "```bash\n",
    "wget http://images.cocodataset.org/zips/train2014.zip\n",
    "unzip train2014.zip\n",
    "rm train2014.zip\n",
    "wget http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n",
    "unzip annotations_trainval2014.zip\n",
    "rm annotations_trainval2014.zip\n",
    "```\n",
    "\n",
    "Let's install the required packages in our environment. We will be using tensorflow 1.14 in this tutorial but it should work in all the tensorflow versions starting from 1.12. But do let us know if you face any hiccups. Install below-given packages before continue\n",
    "\n",
    "```bash\n",
    "tensorflow==1.14.0\n",
    "hangar==3.0\n",
    "spacy==2.1.8\n",
    "```\n",
    "\n",
    "One more thing before jumping into the tutorial; We need to download the SpaCy English model `en_core_web_md` which cannot be dynamically loaded. Which means, it must be downloaded with the below command outside this runtime and should reload this runtime.\n",
    "\n",
    "```bash\n",
    "python -m spacy download en_core_web_md\n",
    "```\n",
    "\n",
    "Once all the dependencies are installed and loaded, we can start building our hangar repository\n",
    "\n",
    "\n",
    "### Hangar Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hangar Repo initialized at: hangar_repo/.hangar\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from hangar import Repository\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "repo_path = 'hangar_repo'\n",
    "if not os.path.isdir(repo_path):\n",
    "    os.mkdir(repo_path)\n",
    "\n",
    "repo = Repository(repo_path)\n",
    "repo.init(user_name='hhsecond', user_email='sherin@tensorwerk.com')\n",
    "co = repo.checkout(write=True)\n",
    "\n",
    "img_shape = (299, 299, 3)\n",
    "images_aset = co.arraysets.init_arrayset('images', shape=img_shape, dtype=np.uint8, named_samples=False)\n",
    "captions_aset = co.arraysets.init_arrayset(name='captions', shape=(60,), dtype=np.float, variable_shape=True, named_samples=False)\n",
    "co.commit('arrayset init')\n",
    "co.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store image and captions to hangar repo\n",
    "Each image will be converted to RGB channels with dtype uint8. Each caption will be prepended with `START` token and append with `END` token before converting them to floats. We have another preprocessing stage for images later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load captions file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "annotation_file = 'annotations/captions_train2014.json'\n",
    "with open(annotation_file, 'r') as f:\n",
    "    annotations = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2index(sent):\n",
    "  \"\"\"\n",
    "  Convert sentence to an array of indices using SpaCy\n",
    "  \"\"\"\n",
    "  ids = []\n",
    "  doc = nlp(sent)\n",
    "  for token in doc:\n",
    "    if token.has_vector:\n",
    "        id = nlp.vocab.vectors.key2row[token.norm]\n",
    "    else:\n",
    "        id = sent2index('UNK')[0]\n",
    "    ids.append(id)\n",
    "  return ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run through the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [00:06<00:00, 60664.50it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "all_captions = []\n",
    "all_img_name_vector = []\n",
    "data_path = 'train2014/'\n",
    "limit = 100  # if you are not planning to save the whole dataset to hangar. Zero means whole dataset\n",
    "\n",
    "co = repo.checkout(write=True)\n",
    "images_aset = co.arraysets['images']\n",
    "captions_aset = co.arraysets['captions']\n",
    "i = 0\n",
    "with images_aset, captions_aset:\n",
    "  for annot in tqdm(annotations['annotations']):\n",
    "    if limit and i > limit:\n",
    "      continue\n",
    "    image_id = annot['image_id']\n",
    "    assumed_image_paths = 'COCO_train2014_' + '%012d.jpg' % (image_id)\n",
    "    img_path = os.path.join(data_path, assumed_image_paths)\n",
    "    img = Image.open(img_path)\n",
    "    if img.mode == 'L':\n",
    "      img = img.convert('RGB')\n",
    "    img = img.resize(img_shape[:-1])         \n",
    "    img = np.array(img)\n",
    "    cap = sent2index('sos ' + annot['caption'] + ' eos')\n",
    "    cap = np.array(cap, dtype=np.float)\n",
    "    co.arraysets.multi_add({\n",
    "        images_aset.name: img,\n",
    "        captions_aset.name: cap\n",
    "    })\n",
    "    if i % 1000 == 0 and i != 0:\n",
    "      if co.diff.status() == 'DIRTY':\n",
    "        co.commit(f'Added batch {i}')\n",
    "    i += 1\n",
    "co.commit('Added full data')\n",
    "co.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Images\n",
    "\n",
    "our Image captioning network requires a pre-processed input. We use transfer learning for this with a pretrained InceptionV3 network which is available in Keras. But we have a problem. Preprocessing is costly and we don't want to do it all the time. Since Hangar is flexible enough to create multiple arraysets and let you call the group of arrayset as a `dataset`, it is quite easy to do make a new arrayset for the processed image and we don't have to do the preprocessing online but keep a preprocessed image in the new arrayset in the same repository with the same key. Which means, we have three arraysets in our repository (all three has different samples with the same name)\n",
    "- images\n",
    "- captions\n",
    "- processed_images\n",
    "\n",
    "Although we need only the processed_images for the network, we still keep the bare image in the repository in case we need to look into it later or if we decided to do some other preprocessing instead of InceptionV3 (It is always advised to keep the source truth with you).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
    "new_input = image_model.input\n",
    "hidden_layer = image_model.layers[-1].output\n",
    "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n",
    "\n",
    "\n",
    "def process_image(img):\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = image_features_extract_model(img)\n",
    "    return tf.reshape(img, (-1, img.shape[3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "co = repo.checkout(write=True)\n",
    "images = co.arraysets['images']\n",
    "sample_name = list(images.keys())[0]\n",
    "prototype = process_image(images[sample_name]).numpy()\n",
    "pimages = co.arraysets.init_arrayset('processed_images', prototype=prototype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the pre processed image to the new arrayset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:27<00:00,  3.51it/s]\n"
     ]
    }
   ],
   "source": [
    "with pimages:\n",
    "    for key in tqdm(images):\n",
    "        pimages[key] = process_image(images[key]).numpy()\n",
    "        \n",
    "co.commit('processed image saved')\n",
    "co.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloaders for training\n",
    "We are using Tensorflow to build the network but how do we load this data from hangar repository to tensorflow. A naive option is to run through the samples and load the numpy arrays and pass that to the `sess.run` of tensorflow. But that is quite inefficient. Tensorflow uses multiple threads to load the data to memory and its dataloaders can prefetch the data before-hand so that your training loop doesn't get blocked for loading the data. Also, tensoflow dataloader brings batching, shuffling, etc to the table prebuilt. That's cool but how to load data from hangar to tensorflow using TF dataset? Well, we have `make_tf_dataset` which accepts the list of arraysets as a parameter and returns a TF dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Checking out BRANCH: master with current HEAD: d96cebb880376238538ed6008ac50d09d603cdc1\n"
     ]
    }
   ],
   "source": [
    "from hangar import make_tf_dataset\n",
    "co = repo.checkout()  # we don't need write checkout here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0819 17:50:28.122937 140051810633536 deprecation.py:323] From /home/hhsecond/anaconda3/envs/hangar/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:505: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'hangar.arrayset.ArraysetDataReader'>(repo_pth=hangar_repo/.hangar, aset_name=processed_images, default_schema_hash=f230548212ab, isVar=False, varMaxShape=(64, 2048), varDtypeNum=11, mode=r)\n",
      "<class 'hangar.arrayset.ArraysetDataReader'>(repo_pth=hangar_repo/.hangar, aset_name=captions, default_schema_hash=4d60751421d5, isVar=True, varMaxShape=(60,), varDtypeNum=12, mode=r)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1\n",
    "EPOCHS = 2\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "vocab_size = len(nlp.vocab.vectors.key2row)\n",
    "num_steps = 50\n",
    "\n",
    "\n",
    "captions_dset = co.arraysets['captions']\n",
    "pimages_dset = co.arraysets['processed_images']\n",
    "\n",
    "dataset = make_tf_dataset([pimages_dset, captions_dset], shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padded Batching\n",
    "\n",
    "Batching needs a bit more explanation here since the dataset does not just consist of fixed shaped data. We have two dataset in which one is for captions. As you know captions are sequence = variably shaped. So instead of using `dataset.batch` we need to use `dataset.padded_batch` which takes care of padding the tensors with the longest value in each dimension for each batch. This `padded_batch` needs the shape by which the user needs the batch to be padded. Unless you need customization, you can use the shape stored in the `dataset` object by `make_tf_dataset` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0819 17:50:34.101216 140051810633536 deprecation.py:323] From <ipython-input-22-602572df9ef4>:1: DatasetV1.output_shapes (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.data.get_output_shapes(dataset)`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 2048]), TensorShape([None]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.output_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(1000)\n",
    "dataset = dataset.padded_batch(BATCH_SIZE, padded_shapes=dataset.output_shapes)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([None, 64, 2048]), TensorShape([None, None]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now dataset shape must have the batch dimension as well\n",
    "dataset.output_shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the network\n",
    "So have the dataloaders ready. Now let's build the network for image captioning and start training. Rest of this tutorial is a copy of an [official tensorflow tutorial](https://tensorflow.org/beta/tutorials/text/image_captioning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
    "        # hidden shape == (batch_size, hidden_size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        # score shape == (batch_size, 64, hidden_size)\n",
    "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "        # attention_weights shape == (batch_size, 64, 1)\n",
    "        # you get 1 at the last axis because you are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "\n",
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # Since you have already extracted the features and dumped it using pickle\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        # shape after fc == (batch_size, 64, embedding_dim)\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "    def call(self, x, features, hidden):\n",
    "        # defining attention as a separate model\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "        # shape == (batch_size, max_length, hidden_size)\n",
    "        x = self.fc1(output)\n",
    "        # x shape == (batch_size * max_length, hidden_size)\n",
    "        x = tf.reshape(x, (-1, x.shape[2]))\n",
    "        # output shape == (batch_size * max_length, vocab)\n",
    "        x = self.fc2(x)\n",
    "        return x, state, attention_weights\n",
    "\n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "    # initializing the hidden state for each batch\n",
    "    # because the captions are not related from image to image\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "    # TODO: do this dynamically: '<start>' == 2\n",
    "    dec_input = tf.expand_dims([2] * BATCH_SIZE, 1)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        features = encoder(img_tensor)\n",
    "        for i in range(1, target.shape[1]):\n",
    "            # passing the features through the decoder\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "    return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Here we consume the dataset we have made before by looping over it. The dataset returns the image tensor and target tensor (captions) which we will pass to `train_step` for training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "loss_plot = []\n",
    "\n",
    "for epoch in range(0, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "        if batch % 1 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "                epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n",
    "    # storing the epoch and loss value to plot later\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "\n",
    "    print('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
    "                                        total_loss / num_steps))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
